{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anticipated-williams",
   "metadata": {},
   "source": [
    "## Learning to learn by gradient descent by gradient descent\n",
    "\n",
    "In this notebook, we try Self-Attention instead of LSTM netowrks as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import multiprocessing\n",
    "import os.path\n",
    "import csv\n",
    "import copy\n",
    "import joblib\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "sns.set_style(\"white\")\n",
    "from pdb import set_trace as bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "def w(v):\n",
    "    if USE_CUDA:\n",
    "        return v.cuda()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir cache\n",
    "cache = joblib.Memory(location='cache', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta_module import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-crest",
   "metadata": {},
   "source": [
    "## Gradient detach\n",
    "\n",
    "As we perform operations, Pytorch builds the computational graph of the operations we perform. However, there are some variables that we want to detach from the graph at various points, specifically we want to pretend that the **gradients are inputs** (as specified in the previous image) that come from nowhere, instead of coming from the rest of the computational graph as they really do: this means we want to **detach** the gradients from the graph. Likewise, when every 20 steps we perform backpropagation on the optimizer network, we want the current hidden states and cell states, as well as the parameters of the optimizee to \"forget\" that they are dependent on previous steps in the graph. For all of this, I created a function called `detach_var` which creates a new Variable from the current variable's data, and makes sure that its gradients are still kept. This is different from the `.detach()` function in Pytorch which does not quite forget the original graph and also does not guarantee that the gradients will be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_var(v):\n",
    "    var = w(Variable(v.data, requires_grad=True))\n",
    "    var.retain_grad()\n",
    "    return var\n",
    "\n",
    "import functools\n",
    "\n",
    "def rsetattr(obj, attr, val):\n",
    "    pre, _, post = attr.rpartition('.')\n",
    "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
    "\n",
    "# using wonder's beautiful simplification: https://stackoverflow.com/questions/31174295/getattr-and-setattr-on-nested-objects/31174427?noredirect=1#comment86638618_31174427\n",
    "\n",
    "def rgetattr(obj, attr, *args):\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr, *args)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fit(optimizer_net, meta_opt, cost_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, preproc = True, should_train=True):\n",
    "\n",
    "    r\"\"\"\"\n",
    "    \n",
    "    Input variables:\n",
    "\n",
    "    optimizer_net:      optimizer network class\n",
    "    meta_opt:           optimizer method for the optimizer, in this notebook we use ADAM\n",
    "    cost_cls:           cost/loss function\n",
    "    target_to_opt:      optimizee class\n",
    "    optim_it, unroll:   Each epoch is made up of trying to optimize a new random function for 'optim_it' steps, \n",
    "                        but we are doing an update of the optimizer every 'unroll' steps.                       \n",
    "    n_epochs:           number of epochs\n",
    "    out_mul:            learning rate for update parameters in the optimizee network?\n",
    "    params:             number of parameters of the omptimizee network\n",
    "    \"\"\"\n",
    "    \n",
    "    if should_train:\n",
    "        optimizer_net.train()\n",
    "    else:\n",
    "        optimizer_net.eval()\n",
    "        unroll = 1\n",
    "    \n",
    "    target = cost_cls(training=should_train)\n",
    "    optimizee = w(target_to_opt())\n",
    "    n_params = 0\n",
    "    \n",
    "    for name, p in optimizee.all_named_parameters():\n",
    "        n_params += int(np.prod(p.size()))\n",
    "        \n",
    "    all_losses_ever = []\n",
    "\n",
    "    if should_train:\n",
    "        meta_opt.zero_grad()\n",
    "\n",
    "    # Compute the loss of the optimizee and compute the cumulative loss over all iterations.\n",
    "    all_losses = None\n",
    "    for iteration in range(1, optim_it + 1):\n",
    "        loss = optimizee(target)\n",
    "                    \n",
    "        if all_losses is None:\n",
    "            all_losses = loss\n",
    "        else:\n",
    "            all_losses += loss\n",
    "            \n",
    "        # Appends the current individual loss to a file\n",
    "        all_losses_ever.append(loss.data.cpu().numpy())\n",
    "        \n",
    "        # Compute optimizee's backward propagation of the loss and retain_graph to be used when optimizing the optimizer.\n",
    "        loss.backward(retain_graph=should_train)\n",
    "\n",
    "        # Update each parameters and the cell and hidden states by iterating through the optimizee's \"all_named_parameters\".\n",
    "        \n",
    "        result_params = {}\n",
    "        for name, p in optimizee.all_named_parameters():\n",
    "            cur_sz = int(np.prod(p.size()))\n",
    "\n",
    "            # We do this so the gradients are disconnected from the graph but we still get\n",
    "            # gradients from the rest\n",
    "            gradients = detach_var(p.grad.view(cur_sz, 1))\n",
    "            if preproc == True:\n",
    "                gradients = preprocess_gradient(gradients)\n",
    "            \n",
    "            # The gradients are fed to the optimizer network as a flatenned layer (1D)\n",
    "            updates = optimizer_net(gradients)\n",
    "                \n",
    "            # Updated parameters of the optimizee function    \n",
    "            result_params[name] = p + updates.view(*p.size()) * out_mul\n",
    "            \n",
    "            # The resulting variable isn't a leaf, which means it won't retain grads by default.\n",
    "            result_params[name].retain_grad()\n",
    "            \n",
    "            \n",
    "        # Update the optimizer parameters if    \n",
    "        if iteration % unroll == 0:\n",
    "            if should_train:\n",
    "                meta_opt.zero_grad()\n",
    "                all_losses.backward()\n",
    "                meta_opt.step()\n",
    "\n",
    "            # Restart the losses    \n",
    "            all_losses = None\n",
    "\n",
    "            # Train a new random network with the last parameters obtained and reinitialize the grad\n",
    "            optimizee = w(target_to_opt())\n",
    "            optimizee.load_state_dict(result_params)\n",
    "            optimizee.zero_grad()\n",
    "            \n",
    "        else:\n",
    "            for name, p in optimizee.all_named_parameters():\n",
    "                rsetattr(optimizee, name, result_params[name])\n",
    "            assert len(list(optimizee.all_named_parameters()))\n",
    "            \n",
    "    return all_losses_ever\n",
    "\n",
    "def preprocess_gradient(gradients):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.\n",
    "        p       : `p` > 0 is a parameter controlling how small gradients are disregarded \n",
    "    Returns:\n",
    "       `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements\n",
    "       along the nth dimension correspond to the `log output` \\in [-1,1] and the remaining\n",
    "       `d_n` elements to the `sign output`.\n",
    "    \"\"\"\n",
    "    p_threshold  = 10\n",
    "    log = torch.log(torch.abs(gradients))\n",
    "    clamp_log = torch.clamp(log/p_threshold , min = -1.0,max = 1.0)\n",
    "    clamp_sign = torch.clamp(torch.exp(w(torch.Tensor(p_threshold)))*gradients, min = -1.0, max =1.0)\n",
    "    return torch.cat((clamp_log,clamp_sign), dim = -1)\n",
    "\n",
    "\n",
    "@cache.cache\n",
    "def fit_optimizer(cost_cls, target_to_opt, preproc=False, unroll=20, optim_it=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0):\n",
    "\n",
    "    # Call the Transformer\n",
    "    # We need to change this if we are other code.\n",
    "    # It does not work yet.\n",
    "    encoder_layer = w(nn.TransformerEncoderLayer(d_model=512, nhead=1))\n",
    "    optimizer_net = w(nn.TransformerEncoder(encoder_layer, num_layers=2))\n",
    "\n",
    "    # Choose the optimizer that will optimize the Transformer network\n",
    "    # i.e.: the meta-optimizer\n",
    "    meta_opt = optim.Adam(optimizer_net.parameters(), lr=lr)\n",
    "    \n",
    "    best_net = None\n",
    "    best_loss = 100000000000000000\n",
    "    \n",
    "    for _ in tqdm(range(n_epochs), 'epochs'):\n",
    "        for _ in tqdm(range(20), 'iterations'):\n",
    "            do_fit(optimizer_net, meta_opt, cost_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True)\n",
    "        \n",
    "        loss = (np.mean([\n",
    "            np.sum(do_fit(optimizer_net, meta_opt, cost_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=False))\n",
    "            for _ in tqdm(range(n_tests), 'tests')\n",
    "        ]))\n",
    "        \n",
    "        print(loss)\n",
    "        if loss < best_loss:\n",
    "            print(best_loss, loss)\n",
    "            best_loss = loss\n",
    "            best_net = copy.deepcopy(optimizer_net.state_dict())\n",
    "            \n",
    "    return best_loss, best_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-restoration",
   "metadata": {},
   "source": [
    "## Optimizer network: PyTorch Transformer\n",
    "\n",
    "In this new re-implementation we will use a Transformer as Optimizer instead of a LSTM.\n",
    "As of now, I try to use PyTorch Transformer module. However it is not working.\n",
    "\n",
    "What are the advantages of a Transformer over LSTM networks?\n",
    "The transformers are non sequential and use positional embeddings to replace recurrence. On the other hand, LSTM are sequential and depend on the previous hidden and cell states, and therefore not possible to parallelize. Our hypothesis is that transformers can speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer = torch.nn.Transformer(\n",
    "    d_model=512, \n",
    "    nhead=8, \n",
    "    dim_feedforward=2048, \n",
    "    dropout=0.1, \n",
    "    activation='relu')\n",
    "\n",
    "r\"\"\"\"\n",
    "    d_model – the number of expected features in the encoder/decoder inputs (default=512).\n",
    "    nhead – the number of heads in the multiheadattention models (default=8).\n",
    "    num_encoder_layers – the number of sub-encoder-layers in the encoder (default=6).\n",
    "    num_decoder_layers – the number of sub-decoder-layers in the decoder (default=6).\n",
    "    dim_feedforward – the dimension of the feedforward network model (default=2048).\n",
    "    dropout – the dropout value (default=0.1).\n",
    "    activation – the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).\n",
    "    custom_encoder – custom encoder (default=None).\n",
    "    custom_decoder – custom decoder (default=None).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(Optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-appraisal",
   "metadata": {},
   "source": [
    "# Optimizee Network: Quadratic functions\n",
    "\n",
    "The optimizer is supposed to find a 10-element vector called $\\theta$ that, when multiplied by a 10x10 matrix called $W$, is as close as possible to a 10-element vector called $y$. Both $y$ and $W$ are generated randomly. The error is simply the squared error.\n",
    "\n",
    "## Class and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticLoss:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.W = w(Variable(torch.randn(10, 10)))\n",
    "        self.y = w(Variable(torch.randn(10)))\n",
    "        \n",
    "    def get_loss(self, theta):\n",
    "        return torch.sum((self.W.matmul(theta) - self.y)**2)\n",
    "    \n",
    "class QuadOptimizee(MetaModule):\n",
    "    def __init__(self, theta=None):\n",
    "        super().__init__()\n",
    "        self.register_buffer('theta', to_var(torch.zeros(10).cuda(), requires_grad=True))\n",
    "        \n",
    "    def forward(self, target):\n",
    "        return target.get_loss(self.theta)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.theta]\n",
    "    \n",
    "    def all_named_parameters(self):\n",
    "        return [('theta', self.theta)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-chapter",
   "metadata": {},
   "source": [
    "## Find best learning rate for meta_optimizer\n",
    "The experiment below fits various learning rates that are used in the meta_optimizer (ADAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in tqdm([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001], 'all'):\n",
    "    print('Learning rate:', lr)\n",
    "    print(fit_optimizer(QuadraticLoss, QuadOptimizee, lr=lr)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-genesis",
   "metadata": {},
   "source": [
    "The experiment shows that 0.003 is a promising learning rate. It is not the lowest, but we are training on only 20 epochs by default and will then retrain with 100 epochs, so it is good to have a slightly lower learning rate for training for longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-schema",
   "metadata": {},
   "source": [
    "Next, the final model is trained with the learning rate (lr = 0.003) found in the previous block and the number of epochs are increased to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, quad_optimizer = fit_optimizer(QuadraticLoss, QuadOptimizee, lr=0.003, n_epochs=100)\n",
    "print('The transformer model loss with the best found learning rate is: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-hostel",
   "metadata": {},
   "source": [
    "## Find best learning rate for conventional optimizers\n",
    "\n",
    "The following two functions are used to find the best learning rate for conventional optimizers: ADAM, RMSProp, SGD and NAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache.cache\n",
    "def fit_normal(target_cls, target_to_opt, opt_class, n_tests=100, n_epochs=100, **kwargs):\n",
    "    results = []\n",
    "    for i in tqdm(range(n_tests), 'tests'):\n",
    "        target = target_cls(training=False)\n",
    "        optimizee = w(target_to_opt())\n",
    "        optimizer = opt_class(optimizee.parameters(), **kwargs)\n",
    "        total_loss = []\n",
    "        for _ in range(n_epochs):\n",
    "            loss = optimizee(target)\n",
    "            \n",
    "            total_loss.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        results.append(total_loss)\n",
    "    return results\n",
    "\n",
    "def find_best_lr_normal(target_cls, target_to_opt, opt_class, **extra_kwargs):\n",
    "    best_loss = 1000000000000000.0\n",
    "    best_lr = 0.0\n",
    "    for lr in tqdm([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001], 'Learning rates'):\n",
    "        try:\n",
    "            loss = best_loss + 1.0\n",
    "            loss = np.mean([np.sum(s) for s in fit_normal(target_cls, target_to_opt, opt_class, lr=lr, **extra_kwargs)])\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_lr = lr\n",
    "    return best_loss, best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMAL_OPTS = [(optim.Adam, {}), (optim.RMSprop, {}), (optim.SGD, {'momentum': 0.9}), (optim.SGD, {'nesterov': True, 'momentum': 0.9})]\n",
    "OPT_NAMES = ['ADAM', 'RMSprop', 'SGD', 'NAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: the momentum parameter for nesterov was found from the following file: \n",
    "#https://github.com/torch/optim/blob/master/nag.lua  since it is mentioned\n",
    "# n the paper that \"When an optimizer has more parameters than just a learning rate (e.g. decay coefficients for ADAM) \n",
    "#we use the default values from the optim package in Torch7.\"\n",
    "\n",
    "for opt, kwargs in NORMAL_OPTS:\n",
    "    print(find_best_lr_normal(QuadraticLoss, QuadOptimizee, opt, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-group",
   "metadata": {},
   "source": [
    "In the cell below:\n",
    "\n",
    "- QUAD_LRS are the best learning rates obtained for the conventional optimizers\n",
    "- fit_data is initialized to 0 and the third dimension has length equal to all conventional opt. + Transformer\n",
    "- The data is fitted with the best learning rate for conventional optimizers.\n",
    "- The state_dict of the optimizer network found in the previous section is loaded\n",
    "- Why is it fitted again I don't know    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUAD_LRS = [0.1, 0.03, 0.01, 0.01]\n",
    "fit_data = np.zeros((100, 100, len(OPT_NAMES) + 1))\n",
    "for i, ((opt, extra_kwargs), lr) in enumerate(zip(NORMAL_OPTS, QUAD_LRS)):\n",
    "    np.random.seed(0)\n",
    "    fit_data[:, :, i] = np.array(fit_normal(QuadraticLoss, QuadOptimizee, opt, lr=lr, **extra_kwargs))\n",
    "\n",
    "opt = w(Optimizer())\n",
    "opt.load_state_dict(quad_optimizer)\n",
    "np.random.seed(0)\n",
    "fit_data[:, :, len(OPT_NAMES)] = np.array([do_fit(opt, None, QuadraticLoss, QuadOptimizee, 1, 100, 100, out_mul=1.0, should_train=False) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-newfoundland",
   "metadata": {},
   "source": [
    "## Graphical results\n",
    "\n",
    "Here, our results are shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.tsplot(data=fit_data[:, :, :], condition=OPT_NAMES + ['LSTM'], linestyle='--', color=['r', 'b', 'g', 'k', 'y'])\n",
    "ax.lines[-1].set_linestyle('-')\n",
    "ax.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Quadratic functions')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('quadratic_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-working",
   "metadata": {},
   "source": [
    "The results obtain in the paper with the LSTM optimizer are:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-dispute",
   "metadata": {},
   "source": [
    "# Optimizee network: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoss:\n",
    "    def __init__(self, training=True):\n",
    "        dataset = datasets.MNIST(\n",
    "            '/home/chenwy/mnist', train=True, download=True,\n",
    "            transform=torchvision.transforms.ToTensor()\n",
    "        )\n",
    "        indices = list(range(len(dataset)))\n",
    "        np.random.RandomState(10).shuffle(indices)\n",
    "        if training:\n",
    "            indices = indices[:len(indices) // 2]\n",
    "        else:\n",
    "            indices = indices[len(indices) // 2:]\n",
    "\n",
    "        self.loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=128,\n",
    "            sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "\n",
    "        self.batches = []\n",
    "        self.cur_batch = 0 #current batch\n",
    "        \n",
    "    def sample(self):\n",
    "        if self.cur_batch >= len(self.batches):\n",
    "            self.batches = []\n",
    "            self.cur_batch = 0\n",
    "            for b in self.loader:\n",
    "                self.batches.append(b)\n",
    "        batch = self.batches[self.cur_batch]\n",
    "        self.cur_batch += 1\n",
    "        return batch\n",
    "\n",
    "class MNISTNet(MetaModule):\n",
    "    def __init__(self, layer_size=20, n_layers=1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        inp_size = 28*28\n",
    "        self.layers = {}\n",
    "        for i in range(n_layers):\n",
    "            self.layers[f'mat_{i}'] = MetaLinear(inp_size, layer_size)\n",
    "            inp_size = layer_size\n",
    "\n",
    "        self.layers['final_mat'] = MetaLinear(inp_size, 10)\n",
    "        self.layers = nn.ModuleDict(self.layers)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.loss = nn.NLLLoss()\n",
    "\n",
    "    def all_named_parameters(self):\n",
    "        return [(k, v) for k, v in self.named_parameters()]\n",
    "    \n",
    "    def forward(self, loss):\n",
    "        inp, out = loss.sample()\n",
    "        inp = w(Variable(inp.view(inp.size()[0], 28*28)))\n",
    "        out = w(Variable(out))\n",
    "\n",
    "        cur_layer = 0 #current layer\n",
    "        while f'mat_{cur_layer}' in self.layers:\n",
    "            inp = self.activation(self.layers[f'mat_{cur_layer}'](inp))\n",
    "            cur_layer += 1\n",
    "\n",
    "        inp = F.log_softmax(self.layers['final_mat'](inp), dim=1)\n",
    "        l = self.loss(inp, out)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-remark",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
